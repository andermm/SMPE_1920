#+TITLE: LabBook
#+AUTHOR: Anderson Mattheus Maliszewski
#+STARTUP: overview indent
#+TAGS: noexport(n) deprecated(d) 
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

- This is the LabBook for the *Scientic Methodology and Performance
  Evaluation for Experimental Computer* class, in which informations
  about the experimental procedure are described.

* Experimental Project

** Objective
   Evaluate the impact of the network interconnection over parallel
   programs. Both *InfiniBand (IB)* and *Gigabit Ethernet (ETH)*
   interconnections were evaluated, using the same physical cluster of servers, executing the NAS
   Parallel Benchmarks version 3.4 with input *class D* as representative HPC benchmarks. The
   evaluation was made using the *Masssage Passing Interface (MPI)*
   parallel implementation of the original set of benchmarks from the
   NPB suite, which is composed by five kernels *(IS, EP, CG, FT, and
   MG)* and three pseudo-application *(BT, SP, and LU).* They are executed
   using ETH, IB and IP-over-IB with 128 processes over 4 nodes (32 processes per node) in
   the case of IS, EP, CG, FT, MG, and LU (power-of-two). As BT and SP
   require that the number of processes to be a square root, it was
   used 121 processes, with 31 processes in one node and 30 processes
   in the last 3 nodes.
   
** System Information 
   To collect the information of all nodes used in the evaluation, it
   was used the the bash script [[./sys_info_collect.sh]]
   which creates a log output with ORG extension in
   [[./LOGS/env_info.org]].
 
** Design of Experiments
   To execute the benchmarks without any bias, it was used the library
   DoE in R-Studio (script in [[./R/DoE.R]]) to create the Design of Experiments, which is
   located in [[./R/experimental_project.csv]]. In the DoE, it
   was used two factors, *kernels* and *interface*, respectivele,
   randomize option equal a true and 30 replications, totalizing 720
   distincts executions.
** Experiments Execution
   All the experiments were executed using the batch script
   [[./BATCH/nas.batch]] which is subdivided in 3 individual steps. 
 
   1 - Executes the system information collector script ([[./BATCH/nas.batch]]) in the
   four nodes; 
   
   2 - Install the dependencies required to compile the NPB benchmarks,
   download and compile it;
   
   3 - Read the experimental project, prepare the MPI line command for the execution, execute the experiments, and
   finally collect the execution time of the benchmarks.
   
   Previously, the execution script was made to be used in a cluster
   with Slurm job scheduler. However, in the InfiniBand execution, a
   error was reported, regarding memory limitations which need to be
   done to execute the script using *sbatch* command. To overcome this
   problem, the nodes in the cluster were previouly allocated with the
   command ~salloc -p hype -N 4 -J JOB -t 72:00:00~. Finally the the
   execution script was executed as a nomal bash script. As the
   experiments use two distints machine files
   ([[./LOGS/nodes_power_of_2/]] and
   [[./LOGS/nodes_square_root]]) they are not automatically created
   in the execution script.  
** Graphical Analysis 
