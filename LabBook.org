#+TITLE: LabBook
#+AUTHOR: Anderson Mattheus Maliszewski
#+STARTUP: overview indent
#+TAGS: noexport(n) deprecated(d) 
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

This LabBook is for the *Scientific Methodology and Performance
Evaluation for Experimental Computer* class, in which informations
about the experimental procedure are described.

* Experimental Project

** Objective
   Evaluate the impact of the network interconnection over parallel
   programs. Both *InfiniBand (IB)* and *Gigabit Ethernet (ETH)*
   interconnections were evaluated, using the same physical cluster of servers, executing the NAS
   Parallel Benchmarks version 3.4 with input *class D* as representative HPC benchmarks. The
   evaluation was made using the *Messsage Passing Interface (MPI)*
   parallel implementation of the original set of benchmarks from the
   NPB suite, which is composed of five kernels *(IS, EP, CG, FT, and
   MG)* and three pseudo-application *(BT, SP, and LU).* They are executed
   using ETH, IB, and IP-over-IB with 128 processes over 4 nodes (32 processes per node) in
   the case of IS, EP, CG, FT, MG, and LU (power-of-two). As BT and SP
   require that the number of processes to be a square root, it was
   used 121 processes, with 31 processes in one node and 30 processes
   in the last 3 nodes.
   
** System Information 
   For collecting the informations of all nodes used in the evaluation, it
   was used the bash script [[./BATCH/sys_info_collect.sh]],
   which creates a log output with ORG extension in
   [[./LOGS/env_info.org]].
 
** Network Infrastructure Information
Each node has a Mellanox MT27600 Channel Adapter (CA) configured for
the InfiniBand 56 Gb/s 4X FDR ConnectX-3 with firmware version
10.16.1038 and OFED version 4.6-1.0.1.1. All nodes are interconnected
through a  Mellanox SX6036 FDR and a generic de 1 Gbps switch.

** Design of Experiments
   To execute the benchmarks without any bias, it was used the library
   DoE in R-Studio (script in [[./R/DoE.R]]) to create the Design of Experiments, which is
   located in [[./R/experimental_project.csv]]. In the DoE, it
   was used two factors, *kernels* and *interface*, respectively,
   with 30 randomized replications, totalizing 720
   distinct executions.
** Experiments Execution
   All the experiments were executed using the batch script
   [[./BATCH/nas.batch]], which is subdivided into 3 individual steps. 
 
   1 - Executes the system information collector script ([[./BATCH/nas.batch]]) in the
   four nodes; 
   
   2 - Install the dependencies required to compile the NPB benchmarks,
   download and compile it;
   
   3 - Read the experimental project, prepare the MPI line command for the execution, execute the experiments, and
   finally collect the execution time of the benchmarks.
   
   Previously, the execution script was made to be used in a cluster
   with Slurm job scheduler. However, in the InfiniBand execution, an
   error was reported regarding IB memory limitations, which need to be
   done to execute the script using *sbatch* command. To overcome this
   problem, the nodes in the cluster were previouly allocated with the
   command ~salloc -p hype -N 4 -J JOB -t 72:00:00~. Finally the
   execution script was executed as a normal bash script. As the
   experiments use two distint machine files
   ([[./LOGS/nodes_power_of_2/]] and
   [[./LOGS/nodes_square_root]]) they are not automatically created
   in the execution script.  
** Graphical Analysis 
#+begin_src R :results output :session *R* :exports both
options(crayon.enabled=FALSE)
library(tidyverse)
read_csv("LOGS/npb.02-11-2019.21h37m51s.csv", progress=FALSE) -> df
df
#+end_src

